blueprint_name: g2-gke-<your-name>

vars:
  project_id: ## Set GCP Project ID Here ##
  deployment_name: g2-gke-<your-name>
  region: asia-southeast1
  zone: asia-southeast1-a # g2 machine has better availability in this zone
  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <authorized_cidr>/32 # curl -s httpbin.org/ip | jq -r '.origin'
  gcp_public_cidrs_access_enabled: false

deployment_groups:
  - group: primary
    modules:
      - id: network
        source: modules/network/vpc
        settings:
          subnetworks:
            - subnet_name: $(vars.deployment_name)-subnet
              subnet_region: $(vars.region)
              subnet_ip: 10.0.0.0/24
              subnet_flow_logs: true
              subnet_flow_logs_sampling: 1.0
          secondary_ranges_list:
            - subnetwork_name: $(vars.deployment_name)-subnet
              ranges:
                - range_name: pods
                  ip_cidr_range: 10.4.0.0/14
                - range_name: services
                  ip_cidr_range: 10.0.32.0/20
          enable_cloud_nat: true

      - id: private_service_access
        source: community/modules/network/private-service-access
        use: [network]

      - id: node_pool_service_account
        source: community/modules/project/service-account
        settings:
          name: gke-np-sa
          project_roles:
            - logging.logWriter
            - monitoring.metricWriter
            - monitoring.viewer
            - stackdriver.resourceMetadata.writer
            - storage.objectViewer
            - artifactregistry.reader

      - id: workload_service_account
        source: community/modules/project/service-account
        settings:
          name: gke-wl-sa
          project_roles:
            - logging.logWriter
            - monitoring.metricWriter
            - monitoring.viewer
            - stackdriver.resourceMetadata.writer
            - storage.objectAdmin
            - artifactregistry.reader

      - id: gke_cluster
        source: modules/scheduler/gke-cluster
        use: [network, workload_service_account]
        settings:
          release_channel: STABLE
          version_prefix: "1.32."
          enable_filestore_csi: true
          enable_gcsfuse_csi: true
          configure_workload_identity_sa: true # needed when using GCS
          gcp_public_cidrs_access_enabled: $(vars.gcp_public_cidrs_access_enabled)
          enable_private_endpoint: false # Allows for access from authorized public IPs
          cloud_dns_config:
            cluster_dns: "CLOUD_DNS"
            cluster_dns_scope: "CLUSTER_SCOPE"
          master_authorized_networks:
            - display_name: kubectl-access-network
              cidr_block: $(vars.authorized_cidr)
          enable_external_dns_endpoint: false
          enable_dcgm_monitoring: true
        outputs: [instructions]

      - id: tools_pool
        source: modules/compute/gke-node-pool
        use: [gke_cluster, node_pool_service_account]
        settings:
          name: n2
          zones: [$(vars.zone)]
          machine_type: n2-standard-4
          initial_node_count: 2
          autoscaling_total_max_nodes: 3
          autoscaling_total_min_nodes: 1
          enable_private_nodes: true
          auto_upgrade: true

      - id: g2_pool
        source: modules/compute/gke-node-pool
        use: [gke_cluster, node_pool_service_account]
        settings:
          name: l4
          zones: [$(vars.zone)]
          machine_type: g2-standard-4
          static_node_count: 1
          disk_type: pd-balanced
          enable_private_nodes: true
          auto_upgrade: true

      - id: nvidia-smi-job
        source: modules/compute/gke-job-template
        use: [g2_pool]
        settings:
          name: nvidia-smi-job
          image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
          command:
            - nvidia-smi
          node_count: 1
          has_gpu: [true]
        outputs: [instructions]

      ### Google Cloud Storage ###

      - id: training_bucket
        source: community/modules/file-system/cloud-storage-bucket
        settings:
          local_mount: /training-data
          name_prefix: training
          random_suffix: true
          force_destroy: false
          enable_hierarchical_namespace: true

      - id: checkpoint_bucket
        source: community/modules/file-system/cloud-storage-bucket
        settings:
          local_mount: /checkpoint-data
          name_prefix: checkpoint
          random_suffix: true
          force_destroy: false
          enable_hierarchical_namespace: true

      # Create a remote mount of training_bucket using
      # mount options optimized for reading training data.
      # Based on Source of truth https://github.com/GoogleCloudPlatform/gcsfuse/blob/d1373b665b7f60e98856d2181f1193396ef16427/samples/gke-csi-yaml/gpu/checkpointing-pv.yaml#L15
      # Some of the options might be available only on latest GKE version, please check the cluster version to meet the required version https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-storage-fuse-csi-driver-perf
      - id: gcs-training
        source: modules/file-system/pre-existing-network-storage
        settings:
          remote_mount: $(training_bucket.gcs_bucket_name)
          local_mount: /training-data
          fs_type: gcsfuse
          mount_options: >-
            implicit-dirs,
            metadata-cache:ttl-secs:-1,
            metadata-cache:stat-cache-max-size-mb:-1,
            metadata-cache:type-cache-max-size-mb:-1,
            file-cache:max-size-mb:-1,
            file-cache:cache-file-for-range-read:true

      # Create a remote mount of checkpoint_bucket using mount
      # options optimized for writing and reading checkpoint data.
      # Based on Source of truth https://github.com/GoogleCloudPlatform/gcsfuse/blob/d1373b665b7f60e98856d2181f1193396ef16427/samples/gke-csi-yaml/gpu/checkpointing-pv.yaml#L15
      # Some of the options might be available only on latest GKE version, please check the cluster version to meet the required version https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-storage-fuse-csi-driver-perf
      - id: gcs-checkpointing
        source: modules/file-system/pre-existing-network-storage
        settings:
          remote_mount: $(checkpoint_bucket.gcs_bucket_name)
          local_mount: /checkpoint-data
          fs_type: gcsfuse
          mount_options: >-
            implicit-dirs,
            metadata-cache:ttl-secs:-1,
            metadata-cache:stat-cache-max-size-mb:-1,
            metadata-cache:type-cache-max-size-mb:-1,
            file-cache:max-size-mb:-1,
            file-cache:cache-file-for-range-read:true,
            file-cache:enable-parallel-downloads:true,
            rename-dir-limit=200000

      # Persistent Volume for training data
      - id: training-pv
        source: modules/file-system/gke-persistent-volume
        use: [gcs-training, gke_cluster]
        settings:
          gcs_bucket_name: $(training_bucket.gcs_bucket_name)
          capacity_gb: 1000000

      # Persistent Volume for checkpoint data
      - id: checkpointing-pv
        source: modules/file-system/gke-persistent-volume
        use: [gcs-checkpointing, gke_cluster]
        settings:
          gcs_bucket_name: $(checkpoint_bucket.gcs_bucket_name)
          capacity_gb: 1000000

      ### Filestore ###

      - id: filestore
        source: modules/file-system/filestore
        settings:
          zone: $(vars.zone)
          local_mount: /shared
        use: [network, private_service_access]

      - id: shared-filestore-pv
        source: modules/file-system/gke-persistent-volume
        use: [gke_cluster, filestore]

      ### Shared Storage Job ###

      - id: shared-fs-job
        source: modules/compute/gke-job-template
        use:
          - gke_cluster
          - shared-filestore-pv
          - training-pv
        settings:
          name: shared-fs-job
          image: bash
          command:
            - bash
            - -c
            - |
              echo "Set up job folders"
              shopt -s extglob; JOB=${HOSTNAME%%-+([[:digit:]])}
              mkdir /training-data/${JOB}/ -p; mkdir /shared/${JOB}/ -p;

              echo "Writing a seed file (${JOB}/${JOB_COMPLETION_INDEX}.dat) to GCS"
              dd if=/dev/urandom of=/training-data/${JOB}/${JOB_COMPLETION_INDEX}.dat bs=1K count=1000

              echo "Copy the seed data from GCS to Filestore"
              cp /training-data/${JOB}/${JOB_COMPLETION_INDEX}.dat /shared/${JOB}/

              echo "Hash the file on Filestore and save to GCS"
              md5sum /shared/${JOB}/${JOB_COMPLETION_INDEX}.dat > /training-data/${JOB}/${JOB_COMPLETION_INDEX}.md5
          node_count: 1
        outputs: [instructions]
